# Vibe Coding Certification: Course Strategy and Outline

## Executive Summary

### The Vision

This is not a course about AI tools. It is a course about **how modern product managers validate intent faster than ever before** -- using vibe coding as the engine.

The course follows a single narrative arc: **The Confidence Line**. Students start in ambiguity, not knowing what to build. Through rapid AI-assisted building, they gain confidence in their direction. By the end, they ship a production-ready product backed by validated evidence.

**The Confidence Line:**

    M1-M2: High Ambiguity  -->  M3-M4: Gaining Clarity  -->  M5-M6: Production Confidence

### The 6 Modules at a Glance

- **M1: Dive In -- Your First Full Cycle** -- No preamble. Build your first prototype in 15 minutes. Show it. Get feedback. Complete a full Build-Show-Learn-Decide loop in one session. Frameworks come after, not before.
- **M2: What Are You Actually Testing?** -- Now that you can build fast, learn to build smart. Problem framing, validation design, divergent prototypes, fidelity mapping. The skill: name the assumption before you open the tool.
- **M3: Precision Prompting** -- As confidence grows, shift from "build me something" to "build me exactly this." Master context layering, agentic prompting, and living prompt packs.
- **M4: From Vibe to Structure** -- The graduation moment. When to stop exploring and start building for real. Living specs, refactoring, debugging, and the PM-to-engineering handoff.
- **M5: Real-World Complexity** -- APIs, auth, data, and edge cases. Bridge the gap between clean prototypes and the messiness of production. Includes existing-product prototyping.
- **M6: Ship It** -- Full group build day. Deploy to a live URL. Present with a stakeholder pitch. Compete. Leave with a deployed product, a Validation Evidence Brief, a living PRD, and your personal project and prompt library.

### Why Now

The current AI Prototyping course has a perception problem: it feels like a Lovable tutorial. Student feedback calls it "shallow," "too basic," and "a long tool demo." But the underlying methodology has good bones -- it's buried under tool-specific content that becomes outdated every few weeks. A rebrand and restructure around a durable, tool-agnostic framework will fix the root cause, not just the symptoms.

### Why This Wins

No other course on the market -- Reforge, Maven, Harvard, Coursera, or Zero to Mastery -- teaches the **complete journey from ambiguity to production**. They each cover a slice. We cover the arc.

---

## The Problem: What's Broken and Why

### From Students (3 cohorts of feedback)

1. **"Felt like a long Lovable tutorial."** The course over-coupled to one tool. When Lovable changes (which happens every few days), the content breaks. Students wanted methodology, not a tool walkthrough.
2. **"Content was shallow / too basic."** Senior PMs don't need "what is AI" slides. They need advanced frameworks for validating intent, debugging production issues, and working with engineering teams.
3. **"Only one person got to build."** Group labs meant one person prototyped while everyone else watched. Engagement dropped. The Agents and AIPC courses already solved this with individual-first, group-second lab design.
4. **"No path from prototype to production."** Students built a thing and stopped. They wanted to know: how does this become real? How do I deploy it? How do I present it to leadership?
5. **"Too much lecture, not enough hands-on."** The highest-rated moments were always the labs. The lowest-rated were extended slide presentations.

### From Internal Team (Dana kickoff, Feb 9)

6. **The depth trap.** Every time feedback said "go deeper," the only option was to explain more tool-specific mechanics -- which made the course *more* of a tutorial, not less. Depth must come from methodology, not tool instructions.
7. **Instructor variance dominated outcomes.** Two content revamps were never properly tested because instructors didn't follow the plan. The content must be instructor-proof: self-guiding labs with clear timing and outcomes.
8. **Tool obsolescence is a constant threat.** Lovable updates models, features, and UI regularly. Any screen-by-screen instructions will be outdated within weeks. The framework must be the durable layer.

### The Root Cause

The course tried to teach a tool. It should be teaching a **process**. The process is: validate your intent as fast as possible using whatever AI tools are available. The tool doesn't matter. If it's Lovable today and something else tomorrow, the process stays the same.

---

## The Solution: The Confidence Line

> See: Images/AI Vibe Coding - The Confidence Lin.png

### Core Thesis

Every product initiative starts in ambiguity and needs to move toward confidence. In the past, this journey required disconnected tools: Figma for mockups, Miro for brainstorming, Google Docs for specs, then engineering for builds. Vibe coding collapses all of these into a single continuous flow. The *process* hasn't changed. The *tools* have unified.

**The Confidence Line:**

    M1-M2: High Ambiguity  -->  M3-M4: Gaining Clarity  -->  M5-M6: Production Confidence

- **Left side (M1-M2):** Ambiguity is high. You don't know what to build. M1 throws you in the water -- you build, show, learn, and decide in one session. M2 teaches you to swim with intent -- problem framing, assumption mapping, and validation design so every build answers a real question.
- **Middle (M3-M4):** Direction is clearer. You shift from exploration to precision -- structured specs, deliberate architecture, and AI-directed building.
- **Right side (M5-M6):** Confidence is high. You harden the prototype into a production-ready product with real integrations, deployment, and scale.

### Key Design Principles

1. **Tool-agnostic.** Teach the process and mindset. Demo across Cursor, Lovable, Bolt, Replit, v0, etc. Students choose their tool. The methodology is the durable layer; tools are interchangeable.
2. **Advanced from minute one.** No "what is AI" slides. Open with a live build that makes the room gasp. Every module assumes senior PM competency.
3. **80/20 hands-on ratio.** Labs dominate. Lectures exist only to frame the next exercise. Students build, not watch.
4. **Individual practice THEN group project.** Borrow the Agents/AIPC course model. Every student builds individually first, then they come together for group work. Solves the "only one person prototypes" problem.
5. **Real-world messiness.** Every module includes a "Break It" exercise -- deliberate non-happy-path scenarios, debugging, and recovery.
6. **Existing product lens.** Labs offer both "0-to-1" and "enhance an existing product" tracks.
7. **Living documents over static specs.** PRDs as guardrails that evolve alongside the prototype.
8. **Validation over vanity.** Every lab asks "what did you learn?" not "what did you build?"
9. **Future-proof for agentic AI.** Agent-mode workflows, MCP integrations, and multi-agent patterns prepare students for 2026-2027, not 2024.
10. **Reuse over recreate.** ~50-60% of existing content has good bones. Restructure and rebrand, don't rebuild from scratch.
11. **Students leave with a usable artifact.** Not just a certificate -- a personal project, a prompt library, and a living PRD they can apply on day one.
12. **Instructor-proof design.** Labs are self-guiding with clear step-by-step flows, built-in timing, and outcomes that don't depend on instructor energy.

---

## What We Learned from the Competition

### What to Steal

- **From Reforge:** The "Tool vs Toy" prototype distinction (Ravi Mehta). Prototypes should be decision-making instruments, not impressive demos. Also: their 6-element problem frame (Goal, Problem, Context, Constraints, Success Criteria, Explore) is excellent scaffolding for the validation modules.
- **From Maven (Kalmykov):** The 20% theory / 80% hands-on ratio with 200+ exercises. That ratio gets 4.8/5 ratings.
- **From Harvard:** "Learning by doing" pedagogy. AI tools as accelerants for rigorous customer research, not as the main event. Framing of startups as "experimentation machines."
- **From ZTM:** The "Creative Director" framing -- you direct AI tools like a tech lead, not just accept output. Applied here to product strategy, not just code.

### What Nobody Does (Our 7 Innovations)

1. **The full ambiguity-to-production arc.** Reforge stops at "find solutions to known problems." Maven stops at "build your first app." We follow the entire Confidence Line.
2. **The "graduation moment."** When to stop vibing and start structuring. 73% of vibe-coded apps fail to reach production because teams never make this transition deliberately.
3. **Comprehension debt management.** AI codebases show 8x code duplication. Nobody teaches how to bridge from prototype to maintainable code.
4. **Agentic workflows for product development.** Planner/worker patterns, MCP, multi-agent orchestration -- the industry is moving here fast and courses haven't caught up.
5. **Existing product prototyping.** Almost every course is 0-to-1 greenfield. Senior PMs primarily enhance existing products.
6. **The PM-Engineering collaboration evolution.** Vibe coding changes the handoff: you show engineers working software, not spec documents.
7. **Validation as the outcome, not the build.** The build is the instrument; the validated insight is the prize.

### Competitive Positioning

- **vs. Reforge:** They start at "validated problem." We start at ambiguity. They stop at prototype. We go to deployment.
- **vs. Maven:** They're tool tutorials. We're a methodology course.
- **vs. Harvard:** They have rigor but not practitioner speed. We combine both.
- **vs. Coursera/ZTM:** They teach coding skills. We teach product development judgment.

---

## Detailed Module Breakdowns

### Module 1: Dive In -- Your First Full Cycle

> **Confidence Line position:** Far left. Maximum ambiguity. Learn by doing, name it after.

**Theme:** No preamble. No slides about why vibe coding is amazing. You build your first prototype in the first 15 minutes. By the end of this module, you've completed a full Build-Show-Learn-Decide cycle. The frameworks are introduced *after* you've experienced them, not before.

**Learning Objectives:**

- Complete a full Build-Show-Learn-Decide cycle in a single session
- Experience firsthand how building reveals what thinking alone cannot
- Identify where you sit on the Confidence Line (introduced retroactively after the lab)

**Module Flow:**

The structure is deliberately inverted: DO first, LEARN second. Students experience the power of vibe coding through action, then the instructor names and frames what just happened.

**Opening Demo (12 min):**
The instructor takes a deliberately vague, ambiguous problem statement (e.g., "our enterprise customers are churning after onboarding") and, live in front of the class, vibe-codes three completely different solution directions. Each is a clickable, tangible prototype. No explanation of methodology. No slides. Just: watch this.

**Lab: The Full Cycle (40 min):**

- **Individual build (15 min):** Each student receives a real, messy, ambiguous problem brief. Working alone, they produce 1 prototype direction using any AI tool of their choice. Everyone builds from minute one. No setup lectures.
- **Show and test (15 min):** Students pair up and swap prototypes. Each person walks through their partner's prototype as a "user" while the builder observes. What resonated? What confused? What assumption does this test?
- **Decide (10 min):** Each student writes down: what did I learn? Should I double down, pivot, or kill this direction? Pairs share their decisions with the class. The instructor highlights the best "kill" decision -- celebrating the courage to stop, not just the ability to build.

**Retroactive Framework (15 min):**
NOW the instructor introduces the Confidence Line and key concepts -- after students have lived them:
- "What you just did in 40 minutes used to take 3 weeks. Here's the framework that names what you experienced."
- The Confidence Line: where you just were (far left, high ambiguity) and where the course takes you (far right, production confidence)
- "Tool vs Toy" prototypes (Reforge's Ravi Mehta): if your prototype didn't help you decide something, it's a toy. Did yours help you decide?
- Tool landscape overview: code-first editors (Cursor, Claude Code) vs. visual-first builders (Lovable, Bolt, v0). Quick orientation, not a tutorial.

**"Break It" Exercise (10 min):**
Instructor demonstrates what happens when you vibe-code *without* any problem frame -- an AI that hallucinates features, builds the wrong thing beautifully. Lesson: you can build fast, but fast without direction is waste. That's what M2 fixes.

---

### Module 2: What Are You Actually Testing? -- Problem Framing and Validation Design

> **Confidence Line position:** Still left, but now with a methodology. You know you CAN build fast. Now learn what's WORTH building.

**Theme:** M1 proved you can build fast. M2 teaches you to build smart. Before you open any tool, you need to name the assumption you're testing. If you can't articulate what you'll learn from building it, don't build it.

**Learning Objectives:**

- Frame a product problem with the right constraints before building anything
- Design validation experiments that test specific, named assumptions
- Match prototype fidelity to the question you're trying to answer
- Build 3-5 fundamentally different solution directions, not variations of the same idea

**Key Talking Points:**

- The core question: "What am I actually testing?" If you can't answer this in one sentence before you start building, you're building a toy, not a tool.
- Reforge's problem framing method adapted for validation: Goal, Problem, Context, Constraints, Success Criteria, Explore -- used to frame *what you're testing*, not just what you're building. This is the scaffolding that turns fast building into smart building.
- Assumption mapping: for any product idea, what are the 3 riskiest assumptions? Which one, if wrong, kills the whole thing? Build to test THAT one first.
- Divergent prototypes: build 3-5 fundamentally different solution directions, not variations of the same idea. Prompt for divergence, not convergence. (Most teams ask for "a few options" and get the same thing with different button placements.)
- Fidelity mapping: when do you need a clickable mockup vs. a functional app vs. a styled landing page? Match fidelity to the question you're answering. Don't build a full app when a landing page answers the question.
- Three types of feedback sessions: (1) Internal stakeholders for alignment and blind spots, (2) Team members for feasibility, (3) Users for validation. Each group sees prototypes at different fidelity and answers different questions.
- Validating within existing products: how to prototype an enhancement to a shipped product without touching production code
- Real-world example: using a vibe-coded prototype to kill a feature that had executive sponsorship -- the prototype made the UX problem undeniable

**Lab: The Validation Design Sprint (40 min):**

- **Frame first (10 min):** Students receive a new, different problem brief from M1. Before touching any tool, they must write: (1) the riskiest assumption, (2) what they'll build to test it, (3) what fidelity is needed, (4) what a "validated" vs. "invalidated" result looks like. This is the discipline M1 didn't require.
- **Individual build (15 min):** Now students build -- but with intent. They're building specifically to test their named assumption, at the fidelity level they chose. Not just "build something cool."
- **Peer validation (15 min):** Students swap prototypes. The validator doesn't just "use" it -- they evaluate: does this prototype actually test the assumption the builder claimed? Or did they build something impressive that proves nothing? Structured debrief on what was validated vs. invalidated.

**"Break It" Exercise (10 min):**
Instructor shows a beautiful, high-fidelity prototype that validated absolutely nothing -- it looked great but didn't test the actual risky assumption. Side-by-side with an ugly, low-fidelity prototype that answered the critical question definitively. Lesson: fidelity without intent is vanity. Clarity of question beats quality of build.

---

### Module 3: Precision Prompting -- Communicating Product Intent to AI

> **Confidence Line position:** Moving right. You know your direction. Now you need precision.

**Theme:** As your confidence grows, your prompts must evolve from "build me something" to "build me exactly this, because I know what users need."

**Learning Objectives:**

- Architect multi-step prompting strategies that produce high-fidelity, spec-aligned outputs
- Use context layering, constraint injection, and iterative refinement to direct AI with precision
- Create "living prompt packs" that evolve alongside your product understanding

**Key Talking Points:**

- The prompting maturity curve: exploration prompts (divergent, open) vs. execution prompts (convergent, precise) -- matching your prompt style to your confidence level
- Context layering: how to feed AI tools your PRD, design system, user research, and constraints so it builds what you actually mean. Demonstrate with Cursor project rules, .cursorrules files, and system context patterns.
- Advanced techniques applied to product work: Chain-of-Thought for complex user flows, Self-Consistency for evaluating UI/UX alternatives, Constraint Injection for guardrails
- The "living prompt pack" concept: a collection of reusable, evolving prompt templates that encode your product context -- NOT a static document, but a dynamic toolkit
- Agentic prompting: the shift from "prompt and wait" to "delegate and supervise." Introduction to agent mode, multi-step task delegation, and how to direct AI agents like a tech lead (inspired by ZTM's "Creative Director" framing but applied to product strategy, not just code)
- Prompt debugging: when the AI builds the wrong thing, how to diagnose whether the problem is your prompt, your context, or the tool's limitation. The Reforge heuristic: if you're 5+ prompts deep on the same issue, the problem isn't the tool -- it's your framing.

**Lab: The Precision Build (40 min):**

- **Individual build (25 min):** Students receive a moderately detailed product brief (clearer than M1's ambiguous brief -- simulating the "gained some confidence" stage). Each student builds individually using deliberate, multi-step prompting. Key requirement: document your prompt chain (each prompt, why you wrote it that way, what it produced).
- **Peer review (15 min):** Students swap prompt chains with a partner. Can another person reproduce a similar result using only the prompt chain? Discussion on prompt portability, reusability, and what makes a prompt pack "living" vs. static.

**"Break It" Exercise (10 min):**
Live debugging: instructor takes a student's prototype that went off-rails and walks through diagnosing the prompt failure -- was it missing context? Conflicting constraints? Wrong tool for the job?

---

### Module 4: From Vibe to Structure -- Building with Intent

> **Confidence Line position:** The inflection point. This is where exploration becomes commitment.

**Theme:** There's a moment where you stop exploring and start building for real. This module is about recognizing that moment and shifting your approach.

**Learning Objectives:**

- Recognize the "graduation moment" when a prototype should become a structured product
- Create living specs (PRDs) that evolve alongside your builds rather than preceding them
- Use AI to refactor exploratory code into maintainable, structured codebases
- Debug and iterate on prototypes with precision rather than guesswork

**Key Talking Points:**

- The graduation moment: signals that you've validated enough and it's time to commit to a direction. This is the single most important judgment call in vibe coding, and it's what separates senior practitioners from beginners. (Nobody else teaches this -- our biggest differentiation.)
- The 73% failure stat: research shows 73% of vibe-coded apps fail to reach production. The #1 reason: teams never deliberately transition from exploration to structure. 8x code duplication, 153% more architectural problems, "comprehension debt" where you build faster than you understand.
- Living specs: the PRD is no longer a document you write before building -- it's a document that captures what you've learned FROM building. Show how to extract a PRD from a working prototype. (Addresses the old course's PRD confusion feedback directly.)
- Refactoring vibe code: your exploratory prototype is working but messy. How to use AI tools to restructure it -- component architecture, naming conventions, separation of concerns
- Advanced debugging: reading code (even if you're not an engineer), using AI assistants (Claude, ChatGPT) to diagnose and fix issues, understanding error messages, knowing when to dive into code vs. when to re-prompt
- Mode mastery: Chat mode vs. Agent mode vs. manual editing -- when to use each and why. Introduction to planner/worker agent patterns for larger builds.
- Working with engineering teams: how vibe-coded prototypes change the PM-to-eng handoff conversation. You're no longer handing off a spec -- you're handing off a working reference implementation with a living spec.

**Lab: The Refactor Challenge (40 min):**

- **Individual build (25 min):** Each student takes their M3 prototype and "graduates" it -- writes the living PRD extracted from what they built, then uses AI to refactor the codebase into something structured. Each student practices debugging individually (instructor provides a "bug injection" exercise they run against their own prototype).
- **Group share (15 min):** Small groups (3-4) compare their living PRDs and refactored codebases. What did each person prioritize in their spec? How did different tools handle the refactoring? Deliverable: a clean, documented prototype with a living PRD that an engineer could understand.

**"Break It" Exercise (10 min):**
Instructor shows a real example of what happens when you never graduate from vibe coding -- a product that grew organically with no structure until it became unmaintainable. Lesson: vibe coding is the start, not the end.

---

### Module 5: Real-World Complexity -- APIs, Data, and Edge Cases

> **Confidence Line position:** Right side. High confidence, now hardening for reality.

**Theme:** Real products live in messy ecosystems. This module bridges the gap between a clean prototype and the complexity of production.

**Learning Objectives:**

- Integrate real APIs and external services into vibe-coded prototypes
- Handle authentication, data persistence, and state management
- Design for edge cases and failure modes, not just the happy path
- Prototype enhancements to existing products (not just greenfield)

**Key Talking Points:**

- The integration reality: your prototype works in isolation, but real products talk to APIs, store data, handle authentication, and deal with failures
- API integration patterns: connecting to real services (Supabase, Firebase, third-party APIs) through AI-assisted development
- MCP (Model Context Protocol) and external tool integration: the future of AI-powered development is tools that talk to other tools. Show how MCP servers let AI agents interact with databases, APIs, and external services directly -- this is where the industry is heading in 2026-2027.
- The "existing product" use case: how to prototype a new feature for a shipped product -- mocking existing APIs, simulating production data, testing within constraints. (This is what Reforge Build calls "1-to-N work" -- and no course teaches the methodology.)
- Edge case thinking: what happens when the API is down? When the user enters garbage? When there are 10,000 items instead of 10? Use AI to generate edge case tests.
- Security and data basics: what PMs need to know about handling real user data, even in prototypes. Common vulnerabilities in AI-generated code (missing rate limiting, JWT manipulation, password reset flows).

**Lab: The Integration Sprint (40 min):**

- **Individual build (30 min):** Students choose one of two tracks and build individually:
  - **Track A (0-to-1):** Add real API integrations (auth, data storage, external service) to their running prototype
  - **Track B (Existing Product):** Given a mock "existing product" API spec, build a prototype enhancement that integrates with the simulated existing system
- **Chaos round (10 min):** Instructor triggers a "chaos event" (API goes down, data format changes). Students must handle the failure gracefully in their individual builds. Group discussion on what broke and why.

**"Break It" Exercise (10 min):**
Class exercise: instructor deploys a prototype with zero error handling. Students, as a group, list all the ways a real user could break it. Discussion: how do you prioritize which edge cases to handle?

---

### Module 6: Ship It -- Deployment, Scale, and Stakeholder Buy-In

> **Confidence Line position:** Far right. Maximum confidence. Time to ship and present.

**Theme:** A prototype that lives on your laptop changes nothing. This module takes your work from "demo" to "deployed" and teaches you how to present it for maximum impact.

**Learning Objectives:**

- Deploy a vibe-coded product to a live URL using modern deployment tools
- Understand the basics of scale: what happens when real users hit your product
- Present your product with a compelling narrative that drives stakeholder decisions
- Build a personal vibe coding practice for continued use after the course

**Key Talking Points:**

- Deployment demystified: from localhost to live URL in minutes (Vercel, Netlify, Supabase hosting, etc.)
- Scale awareness: what changes when 1 user becomes 100 becomes 10,000 -- connection pools, rate limits, caching (PM-level awareness, not engineering depth)
- Quality gates before shipping: a PM's checklist for "is this production-worthy?" -- error handling coverage, basic security review, performance under load, accessibility baseline. Use AI to run these checks.
- The stakeholder pitch: framing your vibe-coded product as *evidence for a product decision*, not just a "cool demo." Reforge's insight applies: prototypes are questions, not answers. Your pitch should present the evidence, not just the feature.
- Building your ongoing practice: how to integrate vibe coding into your daily PM workflow -- discovery, stakeholder alignment, eng handoff, user testing
- The future of PM-as-builder: agentic AI is making it possible for PMs to maintain and evolve products post-launch, not just prototype them. Multi-agent workflows, background agents, autonomous testing -- preview of where this capability is heading.
- The full Confidence Line revisited: from ambiguity through validation through production -- students map their own journey

**Lab: Group Build Day (100 min):**

This module follows the Agents course model: after 5 modules of individual practice, students come together in small groups (3-4 max) for a full build session. This is where the group project lives.

- **Group build sprint (50-60 min):** Groups receive a project brief and build from zero together. Each member has their own tool instance and works on a different component/feature simultaneously (unlike the old course where one person built and others watched). They coordinate through a shared living PRD.
- **Deploy (10 min):** Each group deploys their product to a live URL. Real deployment, real URL, shareable with anyone.
- **Present and compete (30 min):** Each group presents with a structured pitch:
  1. The problem (what ambiguity they started with)
  2. The validation journey (what they learned by building)
  3. The product (live demo of the deployed app)
  4. The recommendation (ship it, pivot, or kill it -- and why)
- Class votes on best product AND best pitch (addressing the student feedback requesting a competition element)

**"Break It" Exercise (embedded in presentations):**
During each presentation, the audience tries to break the live deployed product. Teams must respond to failures live -- demonstrating real product resilience.

---

**Group Deliverables (what each team produces):**

1. **Deployed product** -- a live URL, shareable with anyone. This is the artifact, not the outcome.
2. **Validation Evidence Brief** -- a 1-page document that captures: what assumption did we start with, what did we build to test it, what did we learn, and what's our recommendation (ship / pivot / kill) with evidence. This is what a PM would actually hand to their VP on Monday morning. The *real* deliverable.
3. **Living PRD** -- the spec they've been evolving since M4, now attached to the final product. Shows the journey from messy exploration to structured intent.

**Individual Deliverables (what each student takes home):**

1. **Personal project** -- built individually across M1-M5, theirs to keep developing after the course.
2. **Prompt library / living prompt pack** -- built across M3-M5, a reusable toolkit they can apply to any product initiative.
3. **Confidence Line self-assessment** -- where are their real work initiatives on the line? What's stuck in ambiguity that could be moved right with a 2-hour build? What's been "vibed" too long and needs graduation? This is the bridge from course to daily practice.

---

## Appendix A: Feedback-to-Solution Mapping

| Feedback Issue | Solution in New Course |
| --- | --- |
| "Felt like a Lovable tutorial" | Tool-agnostic; students choose their tools |
| "Too basic for senior PMs" | No intro AI content; advanced frameworks from minute one |
| "Too much lecture, not enough hands-on" | 80/20 hands-on ratio; labs dominate every module |
| "Groups too large / only one person builds" | Individual builds in M1-M5; groups of 3-4 max in M6 |
| "No non-happy-path scenarios" | "Break It" exercise in every module |
| "Want existing product use cases" | Track B option in M5; existing portfolio lens throughout |
| "PRD confusion" | Living specs concept replaces static PRD |
| "Need prototype-to-production path" | Modules 4-6 explicitly cover this journey |
| "Instructor demo needs to be exciting" | M1 opens with 3-prototype ambiguity demo |
| "Credit/subscription issues" | Tool-agnostic removes single-tool dependency; Lovable Pro partnership for accounts |
| "Want competition element" | M6 includes pitch competition with audience voting |
| "Modules felt disconnected" | Confidence Line narrative threads every module together |
| "Content depth was tool-specific" | Depth comes from methodology, not tool mechanics |

## Appendix B: Competitive Landscape Detail

**Reforge (AI Prototyping + Build tool)**

- Best insight: "Toy vs Tool" prototype framework (Ravi Mehta). Prototypes should be *decision-making instruments*, not impressive demos.
- Best methodology: 4-step process -- Frame problem with constraints, Generate divergent solutions, Pressure test, Get feedback. The 6-element problem frame (Goal, Problem, Context, Constraints, Success Criteria, Explore) is excellent scaffolding.
- Teaching format: 3-5 hrs/week, mix of self-paced + live case studies with featured guests.
- **Their gap:** Assumes the problem is already validated. Skips the ambiguity stage entirely. Stops at prototype -- no path to production.

**Maven (Multiple competing courses)**

- Vladimir Kalmykov: 20% theory / 80% hands-on with 200+ exercises. 4.8/5 ratings.
- Harold Dijkstra: Vibe Coding Bootcamp ships real products with auth, database, Stripe.
- Prashanth Padmanabhan: "Build your first AI app in 2 hours" -- powerful hook.
- **Their gap:** Either beginner-focused or tool-specific. None teaches the strategic progression from ambiguity to production.

**Harvard Business School (Ideation & Prototyping for Innovation)**

- "Learning by doing" with team projects. AI tools as accelerants for research, not the main event.
- Startups as "experimentation machines" framing. Human-centered design methods embedded.
- **Their gap:** Academic pacing. Not practitioner-focused. Doesn't address tooling, debugging, or PM-eng handoff.

**Coursera/Scrimba (Vibe Coding Essentials)**

- Covers tools broadly: Cursor, GitHub Copilot, Claude Code, MCP.
- **Their gap:** Pure tool training. Zero product development methodology.

**Zero to Mastery (Vibe Coding Bootcamp)**

- "Creative Director" framing -- direct AI tools like a tech lead. 158 lessons, 18 hours.
- **Their gap:** Developer-focused. No validation or product strategy layer.

## Appendix C: Timeline and Working Principles

**Immediate timeline:**

- Wednesday Feb 11: Draft outline shared with Dana
- Friday Feb 13: Validation meeting with Carlos (high-level outline + new vision)
- Following week: Iterate on skeleton, begin detailing Module 1
- Feb 24: Dejan starts teaching AI Agents course (bandwidth decreases)

**Working principles:**

- Reuse over recreate -- preserve the 50-60% of existing content that has good bones
- Validate the skeleton before going into module details
- Repo serves as a living content library Dana can maintain and iterate on going forward
- Content must be instructor-proof -- labs are self-guiding with clear step-by-step flows and timing

**Source documents:**

- Insights/AI Prototyping V3 Refinement.txt -- Multi-cohort feedback synthesis and module-by-module diagnosis
- Insights/2025-02-09 Dana-Dejan Kickoff Meeting.md -- Internal alignment on vision, lab structure, and working approach
