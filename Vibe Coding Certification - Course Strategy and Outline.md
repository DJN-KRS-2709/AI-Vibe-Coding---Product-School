# Vibe Coding Certification: Course Strategy and Outline

## Executive Summary

### The Vision

This is not a course about AI tools. It is a course about **how modern product managers validate intent faster than ever before** -- using vibe coding as the engine.

The course follows a single narrative arc: **The Confidence Curve**. Students start in ambiguity, not knowing what to build. Through rapid AI-assisted building, they gain confidence in their direction. By the end, they ship a production-ready product backed by validated evidence.

**The Confidence Curve:**

    M1-M2: High Ambiguity  -->  M3-M4: Gaining Clarity  -->  M5-M6: Production Confidence

### The 6 Modules at a Glance

- **M1: The Vibe Coding Mindset** -- Thriving in ambiguity. Build to think, not to ship. Explore 3 directions in the time it takes to schedule a meeting.
- **M2: Rapid Intent Validation** -- Build to learn. Compress the Build-Measure-Learn cycle from weeks to hours. Validate or kill ideas through tangible prototypes.
- **M3: Precision Prompting** -- As confidence grows, shift from "build me something" to "build me exactly this." Master context layering, agentic prompting, and living prompt packs.
- **M4: From Vibe to Structure** -- The graduation moment. When to stop exploring and start building for real. Living specs, refactoring, debugging, and the PM-to-engineering handoff.
- **M5: Real-World Complexity** -- APIs, auth, data, and edge cases. Bridge the gap between clean prototypes and the messiness of production. Includes existing-product prototyping.
- **M6: Ship It** -- Full group build day. Deploy to a live URL. Present with a stakeholder pitch. Compete. Leave with a portfolio piece and a personal practice.

### Why Now

The current AI Prototyping course has a perception problem: it feels like a Lovable tutorial. Student feedback calls it "shallow," "too basic," and "a long tool demo." But the underlying methodology has good bones -- it's buried under tool-specific content that becomes outdated every few weeks. A rebrand and restructure around a durable, tool-agnostic framework will fix the root cause, not just the symptoms.

### Why This Wins

No other course on the market -- Reforge, Maven, Harvard, Coursera, or Zero to Mastery -- teaches the **complete journey from ambiguity to production**. They each cover a slice. We cover the arc.

---

## The Problem: What's Broken and Why

### From Students (3 cohorts of feedback)

1. **"Felt like a long Lovable tutorial."** The course over-coupled to one tool. When Lovable changes (which happens every few days), the content breaks. Students wanted methodology, not a tool walkthrough.
2. **"Content was shallow / too basic."** Senior PMs don't need "what is AI" slides. They need advanced frameworks for validating intent, debugging production issues, and working with engineering teams.
3. **"Only one person got to build."** Group labs meant one person prototyped while everyone else watched. Engagement dropped. The Agents and AIPC courses already solved this with individual-first, group-second lab design.
4. **"No path from prototype to production."** Students built a thing and stopped. They wanted to know: how does this become real? How do I deploy it? How do I present it to leadership?
5. **"Too much lecture, not enough hands-on."** The highest-rated moments were always the labs. The lowest-rated were extended slide presentations.

### From Internal Team (Dana kickoff, Feb 9)

6. **The depth trap.** Every time feedback said "go deeper," the only option was to explain more tool-specific mechanics -- which made the course *more* of a tutorial, not less. Depth must come from methodology, not tool instructions.
7. **Instructor variance dominated outcomes.** Two content revamps were never properly tested because instructors didn't follow the plan. The content must be instructor-proof: self-guiding labs with clear timing and outcomes.
8. **Tool obsolescence is a constant threat.** Lovable updates models, features, and UI regularly. Any screen-by-screen instructions will be outdated within weeks. The framework must be the durable layer.

### The Root Cause

The course tried to teach a tool. It should be teaching a **process**. The process is: validate your intent as fast as possible using whatever AI tools are available. The tool doesn't matter. If it's Lovable today and something else tomorrow, the process stays the same.

---

## The Solution: The Confidence Curve

### Core Thesis

Every product initiative starts in ambiguity and needs to move toward confidence. In the past, this journey required disconnected tools: Figma for mockups, Miro for brainstorming, Google Docs for specs, then engineering for builds. Vibe coding collapses all of these into a single continuous flow. The *process* hasn't changed. The *tools* have unified.

**The Confidence Curve:**

    M1-M2: High Ambiguity  -->  M3-M4: Gaining Clarity  -->  M5-M6: Production Confidence

- **Left side (M1-M2):** Ambiguity is high. You don't know what to build. Vibe coding lets you explore 10x faster, throw things away cheaply, and validate intent through rapid builds.
- **Middle (M3-M4):** Direction is clearer. You shift from exploration to precision -- structured specs, deliberate architecture, and AI-directed building.
- **Right side (M5-M6):** Confidence is high. You harden the prototype into a production-ready product with real integrations, deployment, and scale.

### Key Design Principles

1. **Tool-agnostic.** Teach the process and mindset. Demo across Cursor, Lovable, Bolt, Replit, v0, etc. Students choose their tool. The methodology is the durable layer; tools are interchangeable.
2. **Advanced from minute one.** No "what is AI" slides. Open with a live build that makes the room gasp. Every module assumes senior PM competency.
3. **80/20 hands-on ratio.** Labs dominate. Lectures exist only to frame the next exercise. Students build, not watch.
4. **Individual practice THEN group project.** Borrow the Agents/AIPC course model. Every student builds individually first, then they come together for group work. Solves the "only one person prototypes" problem.
5. **Real-world messiness.** Every module includes a "Break It" exercise -- deliberate non-happy-path scenarios, debugging, and recovery.
6. **Existing product lens.** Labs offer both "0-to-1" and "enhance an existing product" tracks.
7. **Living documents over static specs.** PRDs as guardrails that evolve alongside the prototype.
8. **Validation over vanity.** Every lab asks "what did you learn?" not "what did you build?"
9. **Future-proof for agentic AI.** Agent-mode workflows, MCP integrations, and multi-agent patterns prepare students for 2026-2027, not 2024.
10. **Reuse over recreate.** ~50-60% of existing content has good bones. Restructure and rebrand, don't rebuild from scratch.
11. **Students leave with a usable artifact.** Not just a certificate -- a personal project, a prompt library, and a living PRD they can apply on day one.
12. **Instructor-proof design.** Labs are self-guiding with clear step-by-step flows, built-in timing, and outcomes that don't depend on instructor energy.

---

## What We Learned from the Competition

### What to Steal

- **From Reforge:** The "Tool vs Toy" prototype distinction (Ravi Mehta). Prototypes should be decision-making instruments, not impressive demos. Also: their 6-element problem frame (Goal, Problem, Context, Constraints, Success Criteria, Explore) is excellent scaffolding for the validation modules.
- **From Maven (Kalmykov):** The 20% theory / 80% hands-on ratio with 200+ exercises. That ratio gets 4.8/5 ratings.
- **From Harvard:** "Learning by doing" pedagogy. AI tools as accelerants for rigorous customer research, not as the main event. Framing of startups as "experimentation machines."
- **From ZTM:** The "Creative Director" framing -- you direct AI tools like a tech lead, not just accept output. Applied here to product strategy, not just code.

### What Nobody Does (Our 7 Innovations)

1. **The full ambiguity-to-production arc.** Reforge stops at "find solutions to known problems." Maven stops at "build your first app." We follow the entire Confidence Curve.
2. **The "graduation moment."** When to stop vibing and start structuring. 73% of vibe-coded apps fail to reach production because teams never make this transition deliberately.
3. **Comprehension debt management.** AI codebases show 8x code duplication. Nobody teaches how to bridge from prototype to maintainable code.
4. **Agentic workflows for product development.** Planner/worker patterns, MCP, multi-agent orchestration -- the industry is moving here fast and courses haven't caught up.
5. **Existing product prototyping.** Almost every course is 0-to-1 greenfield. Senior PMs primarily enhance existing products.
6. **The PM-Engineering collaboration evolution.** Vibe coding changes the handoff: you show engineers working software, not spec documents.
7. **Validation as the outcome, not the build.** The build is the instrument; the validated insight is the prize.

### Competitive Positioning

- **vs. Reforge:** They start at "validated problem." We start at ambiguity. They stop at prototype. We go to deployment.
- **vs. Maven:** They're tool tutorials. We're a methodology course.
- **vs. Harvard:** They have rigor but not practitioner speed. We combine both.
- **vs. Coursera/ZTM:** They teach coding skills. We teach product development judgment.

---

## Detailed Module Breakdowns

### Module 1: The Vibe Coding Mindset -- Thriving in Ambiguity

> **Confidence Curve position:** Far left. Maximum ambiguity.

**Theme:** When you don't know what to build, build something. Vibe coding turns ambiguity from a blocker into an advantage.

**Learning Objectives:**

- Understand why vibe coding is a product development methodology, not just a prototyping trick
- Identify where you sit on the Confidence Curve for any given initiative
- Use rapid AI-assisted builds to explore problem spaces faster than traditional discovery

**Key Talking Points:**

- The old model: research for weeks, write specs, hand off. The new model: build to think, validate to learn, spec what works.
- The Confidence Curve framework -- how to recognize your ambiguity level and match your approach to it
- "Throwaway builds" as a strategic tool -- building things you intend to delete is not waste, it's validation
- "Tool vs Toy" prototypes (inspired by Reforge's Ravi Mehta): every prototype you build should be a *decision-making instrument*, not an impressive demo. If your prototype doesn't help you decide something, it's a toy.
- Real-world example: how vibe coding replaced a 3-week discovery sprint with a 3-hour prototype that got stakeholder buy-in
- Tool landscape overview (Cursor, Lovable, Bolt, Replit, v0, Claude Artifacts, etc.) -- how to pick the right tool for the right stage of confidence. Key distinction: code-first editors (Cursor, Claude Code) vs. visual-first builders (Lovable, Bolt, v0) and when each is appropriate.

**Opening Demo (15 min):**
The instructor takes a deliberately vague, ambiguous problem statement (e.g., "our enterprise customers are churning after onboarding") and, live in front of the class, vibe-codes three completely different solution directions in ~12 minutes. Each is a clickable, tangible prototype. The point: in the time it takes to schedule a brainstorming meeting, you could have three testable hypotheses.

**Lab: The Ambiguity Sprint (30-35 min):**

- **Individual build (20 min):** Each student receives the same real, messy, ambiguous problem brief (intentionally incomplete -- no "right answer"). Working alone, they must produce 1 prototype direction using any AI tool of their choice. Everyone builds, everyone gets hands-on.
- **Pair share (10-15 min):** Students pair up, compare their divergent directions. Discussion: what did your tool choice reveal? What did building reveal that thinking alone couldn't? Class votes on most promising direction.

**"Break It" Exercise (10 min):**
Instructor demonstrates what happens when you vibe-code *without* a clear problem frame -- an AI that hallucinates features, builds the wrong thing beautifully. Lesson: ambiguity is fine, but directionless prompting wastes cycles.

---

### Module 2: Rapid Intent Validation -- Build to Learn, Not to Ship

> **Confidence Curve position:** Still left. Ambiguity remains high, but now you have a direction to test.

**Theme:** The fastest way to validate whether your product direction is right is to put something in people's hands. Vibe coding makes validation cycles nearly free.

**Learning Objectives:**

- Design validation experiments using throwaway prototypes instead of surveys or decks
- Apply the "Intent Validation Loop" -- Build, Show, Learn, Decide
- Distinguish between fidelity levels and match them to validation goals

**Key Talking Points:**

- The Intent Validation Loop: Build a thing, show it to a real person, learn what resonated vs. confused, decide whether to double down, pivot, or kill. This compresses the Build-Measure-Learn cycle from weeks to hours.
- Reforge's problem framing method adapted for validation: Goal, Problem, Context, Constraints, Success Criteria, Explore -- but used to frame *what you're testing*, not just what you're building
- Three types of feedback sessions (borrowed from Reforge, evolved): (1) Internal stakeholders for alignment and blind spots, (2) Team members for feasibility, (3) Users for validation. Each group sees prototypes at different fidelity.
- Fidelity mapping: when do you need a clickable mockup vs. a functional app vs. a styled landing page? Matching fidelity to the question you're trying to answer
- The "divergent prototypes" technique (inspired by Reforge): build 3-5 fundamentally different solution directions, not variations of the same idea. Prompt for divergence, not convergence.
- Validating within existing products: how to prototype an enhancement to a shipped product without touching production code
- Real-world example: using a vibe-coded prototype to kill a feature that had executive sponsorship -- the prototype made the UX problem undeniable

**Lab: The Validation Sprint (40 min):**

- **Individual build (20 min):** Each student takes their M1 direction and builds it into a testable prototype at appropriate fidelity. Everyone is building simultaneously -- no one watches.
- **Peer validation (20 min):** Students pair up and swap prototypes. Each person plays "user" for their partner's prototype, walking through it while the builder observes and takes notes. Structured debrief: what assumption was validated? What was invalidated? What would you change?

**"Break It" Exercise (10 min):**
Instructor shows a beautiful, high-fidelity prototype that validated absolutely nothing -- it looked great but didn't test the actual risky assumption. Lesson: fidelity without intent is vanity.

---

### Module 3: Precision Prompting -- Communicating Product Intent to AI

> **Confidence Curve position:** Moving right. You know your direction. Now you need precision.

**Theme:** As your confidence grows, your prompts must evolve from "build me something" to "build me exactly this, because I know what users need."

**Learning Objectives:**

- Architect multi-step prompting strategies that produce high-fidelity, spec-aligned outputs
- Use context layering, constraint injection, and iterative refinement to direct AI with precision
- Create "living prompt packs" that evolve alongside your product understanding

**Key Talking Points:**

- The prompting maturity curve: exploration prompts (divergent, open) vs. execution prompts (convergent, precise) -- matching your prompt style to your confidence level
- Context layering: how to feed AI tools your PRD, design system, user research, and constraints so it builds what you actually mean. Demonstrate with Cursor project rules, .cursorrules files, and system context patterns.
- Advanced techniques applied to product work: Chain-of-Thought for complex user flows, Self-Consistency for evaluating UI/UX alternatives, Constraint Injection for guardrails
- The "living prompt pack" concept: a collection of reusable, evolving prompt templates that encode your product context -- NOT a static document, but a dynamic toolkit
- Agentic prompting: the shift from "prompt and wait" to "delegate and supervise." Introduction to agent mode, multi-step task delegation, and how to direct AI agents like a tech lead (inspired by ZTM's "Creative Director" framing but applied to product strategy, not just code)
- Prompt debugging: when the AI builds the wrong thing, how to diagnose whether the problem is your prompt, your context, or the tool's limitation. The Reforge heuristic: if you're 5+ prompts deep on the same issue, the problem isn't the tool -- it's your framing.

**Lab: The Precision Build (40 min):**

- **Individual build (25 min):** Students receive a moderately detailed product brief (clearer than M1's ambiguous brief -- simulating the "gained some confidence" stage). Each student builds individually using deliberate, multi-step prompting. Key requirement: document your prompt chain (each prompt, why you wrote it that way, what it produced).
- **Peer review (15 min):** Students swap prompt chains with a partner. Can another person reproduce a similar result using only the prompt chain? Discussion on prompt portability, reusability, and what makes a prompt pack "living" vs. static.

**"Break It" Exercise (10 min):**
Live debugging: instructor takes a student's prototype that went off-rails and walks through diagnosing the prompt failure -- was it missing context? Conflicting constraints? Wrong tool for the job?

---

### Module 4: From Vibe to Structure -- Building with Intent

> **Confidence Curve position:** The inflection point. This is where exploration becomes commitment.

**Theme:** There's a moment where you stop exploring and start building for real. This module is about recognizing that moment and shifting your approach.

**Learning Objectives:**

- Recognize the "graduation moment" when a prototype should become a structured product
- Create living specs (PRDs) that evolve alongside your builds rather than preceding them
- Use AI to refactor exploratory code into maintainable, structured codebases
- Debug and iterate on prototypes with precision rather than guesswork

**Key Talking Points:**

- The graduation moment: signals that you've validated enough and it's time to commit to a direction. This is the single most important judgment call in vibe coding, and it's what separates senior practitioners from beginners. (Nobody else teaches this -- our biggest differentiation.)
- The 73% failure stat: research shows 73% of vibe-coded apps fail to reach production. The #1 reason: teams never deliberately transition from exploration to structure. 8x code duplication, 153% more architectural problems, "comprehension debt" where you build faster than you understand.
- Living specs: the PRD is no longer a document you write before building -- it's a document that captures what you've learned FROM building. Show how to extract a PRD from a working prototype. (Addresses the old course's PRD confusion feedback directly.)
- Refactoring vibe code: your exploratory prototype is working but messy. How to use AI tools to restructure it -- component architecture, naming conventions, separation of concerns
- Advanced debugging: reading code (even if you're not an engineer), using AI assistants (Claude, ChatGPT) to diagnose and fix issues, understanding error messages, knowing when to dive into code vs. when to re-prompt
- Mode mastery: Chat mode vs. Agent mode vs. manual editing -- when to use each and why. Introduction to planner/worker agent patterns for larger builds.
- Working with engineering teams: how vibe-coded prototypes change the PM-to-eng handoff conversation. You're no longer handing off a spec -- you're handing off a working reference implementation with a living spec.

**Lab: The Refactor Challenge (40 min):**

- **Individual build (25 min):** Each student takes their M3 prototype and "graduates" it -- writes the living PRD extracted from what they built, then uses AI to refactor the codebase into something structured. Each student practices debugging individually (instructor provides a "bug injection" exercise they run against their own prototype).
- **Group share (15 min):** Small groups (3-4) compare their living PRDs and refactored codebases. What did each person prioritize in their spec? How did different tools handle the refactoring? Deliverable: a clean, documented prototype with a living PRD that an engineer could understand.

**"Break It" Exercise (10 min):**
Instructor shows a real example of what happens when you never graduate from vibe coding -- a product that grew organically with no structure until it became unmaintainable. Lesson: vibe coding is the start, not the end.

---

### Module 5: Real-World Complexity -- APIs, Data, and Edge Cases

> **Confidence Curve position:** Right side. High confidence, now hardening for reality.

**Theme:** Real products live in messy ecosystems. This module bridges the gap between a clean prototype and the complexity of production.

**Learning Objectives:**

- Integrate real APIs and external services into vibe-coded prototypes
- Handle authentication, data persistence, and state management
- Design for edge cases and failure modes, not just the happy path
- Prototype enhancements to existing products (not just greenfield)

**Key Talking Points:**

- The integration reality: your prototype works in isolation, but real products talk to APIs, store data, handle authentication, and deal with failures
- API integration patterns: connecting to real services (Supabase, Firebase, third-party APIs) through AI-assisted development
- MCP (Model Context Protocol) and external tool integration: the future of AI-powered development is tools that talk to other tools. Show how MCP servers let AI agents interact with databases, APIs, and external services directly -- this is where the industry is heading in 2026-2027.
- The "existing product" use case: how to prototype a new feature for a shipped product -- mocking existing APIs, simulating production data, testing within constraints. (This is what Reforge Build calls "1-to-N work" -- and no course teaches the methodology.)
- Edge case thinking: what happens when the API is down? When the user enters garbage? When there are 10,000 items instead of 10? Use AI to generate edge case tests.
- Security and data basics: what PMs need to know about handling real user data, even in prototypes. Common vulnerabilities in AI-generated code (missing rate limiting, JWT manipulation, password reset flows).

**Lab: The Integration Sprint (40 min):**

- **Individual build (30 min):** Students choose one of two tracks and build individually:
  - **Track A (0-to-1):** Add real API integrations (auth, data storage, external service) to their running prototype
  - **Track B (Existing Product):** Given a mock "existing product" API spec, build a prototype enhancement that integrates with the simulated existing system
- **Chaos round (10 min):** Instructor triggers a "chaos event" (API goes down, data format changes). Students must handle the failure gracefully in their individual builds. Group discussion on what broke and why.

**"Break It" Exercise (10 min):**
Class exercise: instructor deploys a prototype with zero error handling. Students, as a group, list all the ways a real user could break it. Discussion: how do you prioritize which edge cases to handle?

---

### Module 6: Ship It -- Deployment, Scale, and Stakeholder Buy-In

> **Confidence Curve position:** Far right. Maximum confidence. Time to ship and present.

**Theme:** A prototype that lives on your laptop changes nothing. This module takes your work from "demo" to "deployed" and teaches you how to present it for maximum impact.

**Learning Objectives:**

- Deploy a vibe-coded product to a live URL using modern deployment tools
- Understand the basics of scale: what happens when real users hit your product
- Present your product with a compelling narrative that drives stakeholder decisions
- Build a personal vibe coding practice for continued use after the course

**Key Talking Points:**

- Deployment demystified: from localhost to live URL in minutes (Vercel, Netlify, Supabase hosting, etc.)
- Scale awareness: what changes when 1 user becomes 100 becomes 10,000 -- connection pools, rate limits, caching (PM-level awareness, not engineering depth)
- Quality gates before shipping: a PM's checklist for "is this production-worthy?" -- error handling coverage, basic security review, performance under load, accessibility baseline. Use AI to run these checks.
- The stakeholder pitch: framing your vibe-coded product as *evidence for a product decision*, not just a "cool demo." Reforge's insight applies: prototypes are questions, not answers. Your pitch should present the evidence, not just the feature.
- Building your ongoing practice: how to integrate vibe coding into your daily PM workflow -- discovery, stakeholder alignment, eng handoff, user testing
- The future of PM-as-builder: agentic AI is making it possible for PMs to maintain and evolve products post-launch, not just prototype them. Multi-agent workflows, background agents, autonomous testing -- preview of where this capability is heading.
- The full Confidence Curve revisited: from ambiguity through validation through production -- students map their own journey

**Lab: The Group Build + Deploy and Present (full module -- this is the "build day"):**

This module follows the Agents course model: after 5 modules of individual practice, students come together in small groups (3-4 max) for a full build session. This is where the group project lives.

- **Group build sprint (60-70 min):** Groups receive a project brief and build from zero together. Each member has their own tool instance and works on a different component/feature simultaneously (unlike the old course where one person built and others watched). They coordinate through a shared living PRD.
- **Deploy (15 min):** Each group deploys their product to a live URL. Real deployment, real URL, shareable with anyone.
- **Present and compete (30 min):** Each group presents with a structured pitch:
  1. The problem (what ambiguity they started with)
  2. The validation journey (what they learned by building)
  3. The product (live demo of the deployed app)
  4. The recommendation (ship it, pivot, or kill it -- and why)
- Class votes on best product AND best pitch (addressing the student feedback requesting a competition element)

**Student takeaways:** Each student leaves with (1) their individual project built across M1-M5 that they can keep developing, (2) the group project as a portfolio piece, and (3) their personal prompt library/living PRD they can apply on day one back at work.

**"Break It" Exercise (embedded in presentations):**
During each presentation, the audience tries to break the live deployed product. Teams must respond to failures live -- demonstrating real product resilience.

---

## Appendix A: Feedback-to-Solution Mapping

| Feedback Issue | Solution in New Course |
| --- | --- |
| "Felt like a Lovable tutorial" | Tool-agnostic; students choose their tools |
| "Too basic for senior PMs" | No intro AI content; advanced frameworks from minute one |
| "Too much lecture, not enough hands-on" | 80/20 hands-on ratio; labs dominate every module |
| "Groups too large / only one person builds" | Individual builds in M1-M5; groups of 3-4 max in M6 |
| "No non-happy-path scenarios" | "Break It" exercise in every module |
| "Want existing product use cases" | Track B option in M5; existing portfolio lens throughout |
| "PRD confusion" | Living specs concept replaces static PRD |
| "Need prototype-to-production path" | Modules 4-6 explicitly cover this journey |
| "Instructor demo needs to be exciting" | M1 opens with 3-prototype ambiguity demo |
| "Credit/subscription issues" | Tool-agnostic removes single-tool dependency; Lovable Pro partnership for accounts |
| "Want competition element" | M6 includes pitch competition with audience voting |
| "Modules felt disconnected" | Confidence Curve narrative threads every module together |
| "Content depth was tool-specific" | Depth comes from methodology, not tool mechanics |

## Appendix B: Competitive Landscape Detail

**Reforge (AI Prototyping + Build tool)**

- Best insight: "Toy vs Tool" prototype framework (Ravi Mehta). Prototypes should be *decision-making instruments*, not impressive demos.
- Best methodology: 4-step process -- Frame problem with constraints, Generate divergent solutions, Pressure test, Get feedback. The 6-element problem frame (Goal, Problem, Context, Constraints, Success Criteria, Explore) is excellent scaffolding.
- Teaching format: 3-5 hrs/week, mix of self-paced + live case studies with featured guests.
- **Their gap:** Assumes the problem is already validated. Skips the ambiguity stage entirely. Stops at prototype -- no path to production.

**Maven (Multiple competing courses)**

- Vladimir Kalmykov: 20% theory / 80% hands-on with 200+ exercises. 4.8/5 ratings.
- Harold Dijkstra: Vibe Coding Bootcamp ships real products with auth, database, Stripe.
- Prashanth Padmanabhan: "Build your first AI app in 2 hours" -- powerful hook.
- **Their gap:** Either beginner-focused or tool-specific. None teaches the strategic progression from ambiguity to production.

**Harvard Business School (Ideation & Prototyping for Innovation)**

- "Learning by doing" with team projects. AI tools as accelerants for research, not the main event.
- Startups as "experimentation machines" framing. Human-centered design methods embedded.
- **Their gap:** Academic pacing. Not practitioner-focused. Doesn't address tooling, debugging, or PM-eng handoff.

**Coursera/Scrimba (Vibe Coding Essentials)**

- Covers tools broadly: Cursor, GitHub Copilot, Claude Code, MCP.
- **Their gap:** Pure tool training. Zero product development methodology.

**Zero to Mastery (Vibe Coding Bootcamp)**

- "Creative Director" framing -- direct AI tools like a tech lead. 158 lessons, 18 hours.
- **Their gap:** Developer-focused. No validation or product strategy layer.

## Appendix C: Timeline and Working Principles

**Immediate timeline:**

- Wednesday Feb 11: Draft outline shared with Dana
- Friday Feb 13: Validation meeting with Carlos (high-level outline + new vision)
- Following week: Iterate on skeleton, begin detailing Module 1
- Feb 24: Dejan starts teaching AI Agents course (bandwidth decreases)

**Working principles:**

- Reuse over recreate -- preserve the 50-60% of existing content that has good bones
- Validate the skeleton before going into module details
- Repo serves as a living content library Dana can maintain and iterate on going forward
- Content must be instructor-proof -- labs are self-guiding with clear step-by-step flows and timing

**Source documents:**

- Insights/AI Prototyping V3 Refinement.txt -- Multi-cohort feedback synthesis and module-by-module diagnosis
- Insights/2025-02-09 Dana-Dejan Kickoff Meeting.md -- Internal alignment on vision, lab structure, and working approach
