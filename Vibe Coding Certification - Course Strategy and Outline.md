# Vibe Coding Certification: Course Strategy and Outline

## Executive Summary

### The Vision

This is not a course about AI tools. It is a course about **how modern product managers validate intent faster than ever before** -- using vibe coding as the engine.

The course follows a single narrative arc: **The Confidence Line**. Students start in ambiguity, not knowing what to build. Through rapid AI-assisted building, they gain confidence in their direction. By the end, they ship a production-ready product backed by validated evidence.

**The Confidence Line:**

    M1-M2: High Ambiguity  -->  M3-M4: Gaining Clarity  -->  M5-M6: Production Confidence

### Pre-Work + 6 Modules at a Glance

- **Pre-Work (async, ~30 min):** Tool setup, first solo build, Confidence Line primer, Slack intro. Students arrive on Day 1 having already built something. Zero setup friction in M1.
- **M1: Dive In -- Your First Full Cycle** -- No preamble. Build your first prototype in 15 minutes. Show it. Get feedback. Complete a full Build-Show-Learn-Decide loop in one session. Frameworks come after, not before.
- **M2: What Are You Actually Testing?** -- Now that you can build fast, learn to build smart. Problem framing, validation design, divergent prototypes, fidelity mapping -- for both greenfield and existing products. The skill: name the assumption before you open the tool.
- **M3: Precision Prompting** -- As confidence grows, shift from "build me something" to "build me exactly this." Master context layering (including for existing products), agentic prompting, and living prompt packs.
- **M4: From Vibe to Structure** -- The graduation moment. When to stop exploring and start building for real. Extract living specs from your builds. Refactor messy prototypes into structured codebases. Learn to graduate within existing products too.
- **M5: Real-World Complexity** -- APIs, auth, data, and edge cases. Bridge the gap between clean prototypes and the messiness of production. Plus: the PM-to-engineering handoff -- how to present your prototype and living spec so engineering can build it for real.
- **M6: Ship It** -- Full group build day. Deploy to a live URL. Present with a stakeholder pitch. Compete. Leave with a deployed product, a Validation Evidence Brief, a living PRD, and your personal project and prompt library.

### Why Now

The current AI Prototyping course has a perception problem: it feels like a Lovable tutorial. Student feedback calls it "shallow," "too basic," and "a long tool demo." But the underlying methodology has good bones -- it's buried under tool-specific content that becomes outdated every few weeks. A rebrand and restructure around a durable, tool-agnostic framework will fix the root cause, not just the symptoms.

### Why This Wins

No other course on the market -- Reforge, Maven, Harvard, Coursera, or Zero to Mastery -- teaches the **complete journey from ambiguity to production**. They each cover a slice. We cover the arc.

---

## The Problem: What's Broken and Why

### From Students (3 cohorts of feedback)

1. **"Felt like a long Lovable tutorial."** The course over-coupled to one tool. When Lovable changes (which happens every few days), the content breaks. Students wanted methodology, not a tool walkthrough.
2. **"Content was shallow / too basic."** Senior PMs don't need "what is AI" slides. They need advanced frameworks for validating intent, debugging production issues, and working with engineering teams.
3. **"Only one person got to build."** Group labs meant one person prototyped while everyone else watched. Engagement dropped. The Agents and AIPC courses already solved this with individual-first, group-second lab design.
4. **"No path from prototype to production."** Students built a thing and stopped. They wanted to know: how does this become real? How do I deploy it? How do I present it to leadership?
5. **"Too much lecture, not enough hands-on."** The highest-rated moments were always the labs. The lowest-rated were extended slide presentations.

### From Internal Team (Dana kickoff, Feb 9)

6. **The depth trap.** Every time feedback said "go deeper," the only option was to explain more tool-specific mechanics -- which made the course *more* of a tutorial, not less. Depth must come from methodology, not tool instructions.
7. **Instructor variance dominated outcomes.** Two content revamps were never properly tested because instructors didn't follow the plan. The content must be instructor-proof: self-guiding labs with clear timing and outcomes.
8. **Tool obsolescence is a constant threat.** Lovable updates models, features, and UI regularly. Any screen-by-screen instructions will be outdated within weeks. The framework must be the durable layer.

### The Root Cause

The course tried to teach a tool. It should be teaching a **process**. The process is: validate your intent as fast as possible using whatever AI tools are available. The tool doesn't matter. If it's Lovable today and something else tomorrow, the process stays the same.

---

## The Solution: The Confidence Line

> See: Images/AI Vibe Coding - The Confidence Lin.png

### Core Thesis

Every product initiative starts in ambiguity and needs to move toward confidence. In the past, this journey required disconnected tools: Figma for mockups, Miro for brainstorming, Google Docs for specs, then engineering for builds. Vibe coding collapses all of these into a single continuous flow. The *process* hasn't changed. The *tools* have unified.

**The Confidence Line:**

    M1-M2: High Ambiguity  -->  M3-M4: Gaining Clarity  -->  M5-M6: Production Confidence

- **Left side (M1-M2):** Ambiguity is high. You don't know what to build. M1 throws you in the water -- you build, show, learn, and decide in one session. M2 teaches you to swim with intent -- problem framing, assumption mapping, and validation design so every build answers a real question.
- **Middle (M3-M4):** Direction is clearer. You shift from exploration to precision -- structured specs, deliberate architecture, and AI-directed building.
- **Right side (M5-M6):** Confidence is high. You harden the prototype into a production-ready product with real integrations, deployment, and scale.

### Key Design Principles

1. **Tool-agnostic.** Teach the process and mindset. Demo across Cursor, Lovable, Bolt, Replit, v0, etc. Students choose their tool. The methodology is the durable layer; tools are interchangeable.
2. **Advanced from minute one.** No "what is AI" slides. Open with a live build that makes the room gasp. Every module assumes senior PM competency.
3. **80/20 hands-on ratio.** Labs dominate. Lectures exist only to frame the next exercise. Students build, not watch.
4. **Individual practice THEN group project.** Borrow the Agents/AIPC course model. Every student builds individually first, then they come together for group work. Solves the "only one person prototypes" problem.
5. **Real-world messiness.** Every module includes a "Break It" exercise -- deliberate non-happy-path scenarios, debugging, and recovery.
6. **Existing product lens.** Labs offer both "0-to-1" and "enhance an existing product" tracks.
7. **Living documents over static specs.** PRDs as guardrails that evolve alongside the prototype.
8. **Validation over vanity.** Every lab asks "what did you learn?" not "what did you build?"
9. **Future-proof for agentic AI.** Agent-mode workflows, MCP integrations, and multi-agent patterns prepare students for 2026-2027, not 2024.
10. **Reuse over recreate.** ~50-60% of existing content has good bones. Restructure and rebrand, don't rebuild from scratch.
11. **Students leave with a usable artifact.** Not just a certificate -- a personal project, a prompt library, and a living PRD they can apply on day one.
12. **Instructor-proof design.** Labs are self-guiding with clear step-by-step flows, built-in timing, and outcomes that don't depend on instructor energy.

### Recommended Tool Path

The course is tool-agnostic by design, but students and instructors need a clear default. This is our recommendation:

**Default tool: Lovable (visual-first builder)**
- Lowest barrier to entry for non-technical PMs. No terminal, no code editor, no local setup.
- Pro accounts provided via Product School partnership (3 months free). This eliminates the #1 friction point from previous cohorts: credit limits and subscription confusion.
- Used as the primary demo tool by instructors in all live sessions.

**Alternative tools encouraged (students may use any of these interchangeably):**
- **Bolt** -- similar visual-first UX to Lovable, good alternative if Lovable has outages or credit issues
- **v0** -- Vercel ecosystem, generates React components, good for students already in the Vercel world
- **Replit** -- more code-visible than Lovable/Bolt, good bridge for students who want to see what's happening under the hood

**Code-first track (optional, not taught in depth):**
- **Cursor** or **Claude Code** -- for students comfortable with code who want to work in a code editor with AI assistance. Instructor should acknowledge this path exists and is valid, but demos and labs are designed for visual-first tools. Code-first students can follow along with their own tool.

**Instructor demo principle:**
- Each module's live demo uses Lovable as the primary tool.
- At least once per module, the instructor shows a brief "and here's the same thing in [Bolt / v0 / Cursor]" moment to reinforce that the methodology works across tools.
- When a student asks a tool-specific question, redirect to the principle: "That button might move next week. The approach stays the same."

**What we do NOT teach:**
- Tool-specific UI navigation, button locations, settings menus, or account configuration
- Version-specific features ("Lovable just added X") -- if it changes next week, our content shouldn't break
- Workarounds for free-tier limitations (solved by the Pro partnership)

---

## What We Learned from the Competition

### What to Steal

- **From Reforge:** The "Tool vs Toy" prototype distinction (Ravi Mehta). Prototypes should be decision-making instruments, not impressive demos. Also: their 6-element problem frame (Goal, Problem, Context, Constraints, Success Criteria, Explore) is excellent scaffolding for the validation modules.
- **From Maven (Kalmykov):** The 20% theory / 80% hands-on ratio with 200+ exercises. That ratio gets 4.8/5 ratings.
- **From Harvard:** "Learning by doing" pedagogy. AI tools as accelerants for rigorous customer research, not as the main event. Framing of startups as "experimentation machines."
- **From ZTM:** The "Creative Director" framing -- you direct AI tools like a tech lead, not just accept output. Applied here to product strategy, not just code.

### What Nobody Does (Our 7 Innovations)

1. **The full ambiguity-to-production arc.** Reforge stops at "find solutions to known problems." Maven stops at "build your first app." We follow the entire Confidence Line.
2. **The "graduation moment."** When to stop vibing and start structuring. 73% of vibe-coded apps fail to reach production because teams never make this transition deliberately.
3. **Comprehension debt management.** AI codebases show 8x code duplication. Nobody teaches how to bridge from prototype to maintainable code.
4. **Agentic workflows for product development.** Planner/worker patterns, MCP, multi-agent orchestration -- the industry is moving here fast and courses haven't caught up.
5. **Existing product prototyping.** Almost every course is 0-to-1 greenfield. Senior PMs primarily enhance existing products.
6. **The PM-Engineering collaboration evolution.** Vibe coding changes the handoff: you show engineers working software, not spec documents.
7. **Validation as the outcome, not the build.** The build is the instrument; the validated insight is the prize.

### Competitive Positioning

- **vs. Reforge:** They start at "validated problem." We start at ambiguity. They stop at prototype. We go to deployment.
- **vs. Maven:** They're tool tutorials. We're a methodology course.
- **vs. Harvard:** They have rigor but not practitioner speed. We combine both.
- **vs. Coursera/ZTM:** They teach coding skills. We teach product development judgment.

---

## How We Know It's Working: Success Metrics

A redesign without measurable outcomes is just a rebrand. Here's how we'll know this course is actually better.

### The One Number That Matters

**Overall course NPS: 4.5+/5** (up from ~3.8 in the current course). This is the headline metric Carlos and Fernan should watch. Everything below supports it.

### During the Course (Leading Indicators)

These are the instructor's real-time dashboard. If any of these drop, something is wrong and we fix it mid-cohort.

| Metric | Target | Signal If Missed |
| --- | --- | --- |
| Hands-on time ratio | 65%+ of class time in labs | Instructor is lecturing too long; cut teaching, protect labs |
| Individual build completion | 90%+ produce a working prototype each module | Lab is too hard, tool is broken, or brief is confusing |
| Pre-work completion | 85%+ before M1 | Pre-work instructions unclear or sent too late |
| Per-module satisfaction | 8+/10 (Slack poll after each session) | Content isn't landing; review that module's delivery |

### At Course End (Lagging Indicators)

These are the course team's report card. Compare cohort-over-cohort.

| Metric | Target | Why It Matters |
| --- | --- | --- |
| Overall NPS | 4.5+/5 | The benchmark. Current course is ~3.8. |
| "Would recommend to a colleague" | 80%+ | Word-of-mouth is the strongest growth driver |
| Group deployment rate | 100% deploy to live URL | If groups can't deploy, M5-M6 content isn't working |
| Validation Evidence Brief quality | Meets rubric (see Instructor Guide) | The real deliverable -- if these are weak, the methodology didn't transfer |

### After the Course (Impact Indicators -- 30-60 days)

This is the business case. If students aren't using this at work, the redesign didn't stick.

| Metric | Target | How to Measure |
| --- | --- | --- |
| Using vibe coding in actual PM work | 60%+ | 30-day follow-up survey |
| Shared prototype or prompt library with team | 40%+ | 30-day follow-up survey |
| Applied the Confidence Line to a real initiative | 50%+ | 30-day follow-up survey |
| Repeat enrollment in other PS AI courses | Track trend | Internal enrollment data |

### How These Metrics Connect

Leading indicators predict lagging indicators. If build completion is high and satisfaction per module is 8+, the end-of-course NPS will follow. Lagging indicators predict impact indicators. If students leave with strong deliverables (Evidence Brief, prompt library, living PRD), they're more likely to use these tools at work. The 30-day survey closes the loop.

---

## Detailed Module Breakdowns

### Pre-Work (Module 0): Get Set Up -- Async, Before Day 1

> **Confidence Line position:** Before the line begins. Remove all friction so M1 starts at full speed.

**Format:** Async, self-paced. Completed before the first live session. ~30-35 minutes total.

**Purpose:** Every previous cohort lost 15-30 minutes of M1 to account creation, subscription issues, and "how do I use this tool?" questions. Pre-work eliminates that. Students arrive on Day 1 having already built something -- even if it's small.

**Pre-Work Tasks:**

1. **Tool setup (~15 min):** Create a Lovable account (default tool -- Pro accounts provided via Product School partnership). Verify the tool loads and you can generate a basic page. Optionally, explore one alternative: Bolt, v0, or Cursor. The course is tool-agnostic -- Lovable is the default for demos, but you may use any builder throughout.

2. **First solo build (~15 min):** Follow a 3-step micro-prompt sequence provided in the pre-work guide:
   - Prompt 1: "Build me a landing page for [fictional product name]"
   - Prompt 2: "Add a pricing section with three tiers"
   - Prompt 3: "Make it mobile responsive and add a signup form"
   - No methodology. No framing. Just prove the tool works and you can get output. Screenshot or share the link.

3. **Confidence Line primer (~5 min):** Watch a 3-minute video (or read a 1-page explainer) introducing the Confidence Line concept. Then self-assess: "Where is my current real-work initiative on this line? Am I stuck in ambiguity? Already building? Ready for production?"

4. **Slack intro:** Post in the cohort Slack channel: your name, your role, what you built in the pre-work (screenshot or link), and one product problem you're currently wrestling with at work.

**What this solves:**
- Setup friction eating into M1 live time (feedback: "the first hour was all setup")
- Students arriving with zero tool experience and slowing down the class
- Subscription/credit confusion (feedback: "free account doesn't allow us to use it properly")
- Cold-start problem in M1 -- students have already built once, so the M1 lab feels like a second build, not a first

---

### Module 1: Dive In -- Your First Full Cycle

> **Confidence Line position:** Far left. Maximum ambiguity. Learn by doing, name it after.

**Timing (2 hrs total):** Opening demo (12 min) + Lab (40 min) + Retroactive framework (15 min) + Break It (10 min) = **77 min structured, 43 min buffer.** Buffer absorbs: Q&A, tool troubleshooting for students who didn't complete pre-work, slow starters, class discussion, and extended demo if energy is high.

**Theme:** No preamble. No slides about why vibe coding is amazing. You build your first prototype in the first 15 minutes. By the end of this module, you've completed a full Build-Show-Learn-Decide cycle. The frameworks are introduced *after* you've experienced them, not before.

**Learning Objectives:**

- Complete a full Build-Show-Learn-Decide cycle in a single session
- Experience firsthand how building reveals what thinking alone cannot
- Identify where you sit on the Confidence Line (introduced retroactively after the lab)

**Module Flow:**

The structure is deliberately inverted: DO first, LEARN second. Students experience the power of vibe coding through action, then the instructor names and frames what just happened.

**Opening Demo (12 min):**
The instructor takes a deliberately vague, ambiguous problem statement (e.g., "our enterprise customers are churning after onboarding") and, live in front of the class, vibe-codes three completely different solution directions. Each is a clickable, tangible prototype. No explanation of methodology. No slides. Just: watch this.

**Lab: The Full Cycle (40 min):**

- **Individual build (15 min):** Each student receives a real, messy, ambiguous problem brief. Working alone, they produce 1 prototype direction using any AI tool of their choice. Everyone builds from minute one. No setup lectures.
- **Show and test (15 min):** Students pair up and swap prototypes. Each person walks through their partner's prototype as a "user" while the builder observes. What resonated? What confused? What assumption does this test?
- **Decide (10 min):** Each student writes down: what did I learn? Should I double down, pivot, or kill this direction? Pairs share their decisions with the class. The instructor highlights the best "kill" decision -- celebrating the courage to stop, not just the ability to build.

**Retroactive Framework (15 min):**
NOW the instructor introduces the Confidence Line and key concepts -- after students have lived them:
- "What you just did in 40 minutes used to take 3 weeks. Here's the framework that names what you experienced."
- The Confidence Line: where you just were (far left, high ambiguity) and where the course takes you (far right, production confidence)
- "Tool vs Toy" prototypes (Reforge's Ravi Mehta): if your prototype didn't help you decide something, it's a toy. Did yours help you decide?
- Tool landscape overview: code-first editors (Cursor, Claude Code) vs. visual-first builders (Lovable, Bolt, v0). Quick orientation, not a tutorial.

**"Break It" Exercise (10 min):**
Instructor demonstrates what happens when you vibe-code *without* any problem frame -- an AI that hallucinates features, builds the wrong thing beautifully. Lesson: you can build fast, but fast without direction is waste. That's what M2 fixes.

---

### Module 2: What Are You Actually Testing? -- Problem Framing and Validation Design

> **Confidence Line position:** Still left, but now with a methodology. You know you CAN build fast. Now learn what's WORTH building.

**Timing (2 hrs total):** Teaching/talking points (20 min) + Lab (40 min) + Break It (10 min) = **70 min structured, 50 min buffer.** Buffer absorbs: deeper discussion on assumption mapping, extra build time for students who frame slowly, extended peer validation discussions.

**Theme:** M1 proved you can build fast. M2 teaches you to build smart. Before you open any tool, you need to name the assumption you're testing. If you can't articulate what you'll learn from building it, don't build it.

**Learning Objectives:**

- Frame a product problem with the right constraints before building anything
- Design validation experiments that test specific, named assumptions
- Match prototype fidelity to the question you're trying to answer
- Build 3-5 fundamentally different solution directions, not variations of the same idea

**Key Talking Points:**

- The core question: "What am I actually testing?" If you can't answer this in one sentence before you start building, you're building a toy, not a tool.
- Reforge's problem framing method adapted for validation: Goal, Problem, Context, Constraints, Success Criteria, Explore -- used to frame *what you're testing*, not just what you're building. This is the scaffolding that turns fast building into smart building.
- Assumption mapping: for any product idea, what are the 3 riskiest assumptions? Which one, if wrong, kills the whole thing? Build to test THAT one first.
- Divergent prototypes: build 3-5 fundamentally different solution directions, not variations of the same idea. Prompt for divergence, not convergence. (Most teams ask for "a few options" and get the same thing with different button placements.)
- Fidelity mapping: when do you need a clickable mockup vs. a functional app vs. a styled landing page? Match fidelity to the question you're answering. Don't build a full app when a landing page answers the question.
- Three types of feedback sessions: (1) Internal stakeholders for alignment and blind spots, (2) Team members for feasibility, (3) Users for validation. Each group sees prototypes at different fidelity and answers different questions.
- **Validating within existing products (the "1-to-N" lens):** Most senior PMs don't build from zero -- they enhance shipped products. How do you prototype an improvement to something that already exists without touching production code? The framing changes: instead of "what should we build?" it's "what should we change, and will users notice?" Assumption mapping for existing products focuses on adoption risk ("will users find this?") and integration risk ("does this break existing workflows?"), not just desirability.
- Real-world example: using a vibe-coded prototype to kill a feature that had executive sponsorship -- the prototype made the UX problem undeniable

**Lab: The Validation Design Sprint (40 min):**

- **Frame first (10 min):** Students choose one of two problem briefs (a new addition -- giving students both a greenfield and an existing-product option):
  - **Brief A (0-to-1):** A new, ambiguous product problem (different from M1). Frame from scratch.
  - **Brief B (Existing Product):** "Your company's analytics dashboard has a 60% bounce rate on the onboarding flow. Prototype 3 different improvements." Frame within the constraints of an existing product -- what can you change, what can't you touch, and what assumption about user behavior are you testing?
  - For whichever brief they choose, students must write before touching any tool: (1) the riskiest assumption, (2) what they'll build to test it, (3) what fidelity is needed, (4) what a "validated" vs. "invalidated" result looks like. This is the discipline M1 didn't require.
- **Individual build (15 min):** Now students build -- but with intent. They're building specifically to test their named assumption, at the fidelity level they chose. Not just "build something cool."
- **Peer validation (15 min):** Students swap prototypes. The validator doesn't just "use" it -- they evaluate: does this prototype actually test the assumption the builder claimed? Or did they build something impressive that proves nothing? Structured debrief on what was validated vs. invalidated.

**"Break It" Exercise (10 min):**
Instructor shows a beautiful, high-fidelity prototype that validated absolutely nothing -- it looked great but didn't test the actual risky assumption. Side-by-side with an ugly, low-fidelity prototype that answered the critical question definitively. Lesson: fidelity without intent is vanity. Clarity of question beats quality of build.

---

### Module 3: Precision Prompting -- Communicating Product Intent to AI

> **Confidence Line position:** Moving right. You know your direction. Now you need precision.

**Timing (2 hrs total):** Teaching (15 min) + Lab (40 min) + Break It (10 min) = **65 min structured, 55 min buffer.** Buffer absorbs: prompt debugging takes unpredictable time, peer review discussions often run long as students compare prompt chains, and agentic prompting demos may need extra Q&A.

**Theme:** As your confidence grows, your prompts must evolve from "build me something" to "build me exactly this, because I know what users need."

**Learning Objectives:**

- Architect multi-step prompting strategies that produce high-fidelity, spec-aligned outputs
- Use context layering, constraint injection, and iterative refinement to direct AI with precision
- Create "living prompt packs" that evolve alongside your product understanding

**Key Talking Points:**

- The prompting maturity curve: exploration prompts (divergent, open) vs. execution prompts (convergent, precise) -- matching your prompt style to your confidence level
- Context layering: how to feed AI tools your PRD, design system, user research, and constraints so it builds what you actually mean. Demonstrate with Cursor project rules, .cursorrules files, and system context patterns.
- **Context layering for existing products:** When you're building within an existing product, your prompts need to carry that context. Show the difference side-by-side: "Build me a settings page" (generic, standalone) vs. "Build me a settings page that matches our existing design system -- here's the component library, color tokens, and navigation pattern" (contextual, integrated). The second prompt produces something that looks like it *belongs*. For PMs working on shipped products, this is the difference between a prototype stakeholders take seriously and one they dismiss as "not how our product looks."
- Advanced techniques applied to product work: Chain-of-Thought for complex user flows, Self-Consistency for evaluating UI/UX alternatives, Constraint Injection for guardrails
- The "living prompt pack" concept: a collection of reusable, evolving prompt templates that encode your product context -- NOT a static document, but a dynamic toolkit
- Agentic prompting: the shift from "prompt and wait" to "delegate and supervise." Introduction to agent mode, multi-step task delegation, and how to direct AI agents like a tech lead (inspired by ZTM's "Creative Director" framing but applied to product strategy, not just code)
- Prompt debugging: when the AI builds the wrong thing, how to diagnose whether the problem is your prompt, your context, or the tool's limitation. The Reforge heuristic: if you're 5+ prompts deep on the same issue, the problem isn't the tool -- it's your framing.

**Lab: The Precision Build (40 min):**

- **Individual build (25 min):** Students receive a moderately detailed product brief (clearer than M1's ambiguous brief -- simulating the "gained some confidence" stage). Each student builds individually using deliberate, multi-step prompting. Key requirement: document your prompt chain (each prompt, why you wrote it that way, what it produced).
- **Peer review (15 min):** Students swap prompt chains with a partner. Can another person reproduce a similar result using only the prompt chain? Discussion on prompt portability, reusability, and what makes a prompt pack "living" vs. static.

**"Break It" Exercise (10 min):**
Live debugging: instructor takes a student's prototype that went off-rails and walks through diagnosing the prompt failure -- was it missing context? Conflicting constraints? Wrong tool for the job?

---

### Module 4: From Vibe to Structure -- The Graduation Moment

> **Confidence Line position:** The inflection point. This is where exploration becomes commitment.

**Timing (2 hrs total):** Teaching (15 min) + Lab (40 min) + Break It (10 min) = **65 min structured, 55 min buffer.** Buffer absorbs: refactoring is variable-time (some students' codebases are messier than others), PRD extraction discussion, and group share often sparks valuable debate about what to include in a spec.

**Theme:** There's a moment where you stop exploring and start building for real. Recognizing that moment -- and knowing what to do when it arrives -- is the single most important judgment call in vibe coding. This module teaches three tightly linked skills: recognize the graduation moment, extract a living spec from what you've built, and refactor your messy prototype into something structured.

**Core skill:** The Graduation Moment + Living Specs + Refactoring. These three are inseparable: you recognize it's time to commit, you capture what you've learned into a spec, and you clean up the code to match.

**Learning Objectives:**

- Recognize the signals that a prototype should graduate from exploration to structured product
- Extract a living spec (PRD) from a working prototype -- capturing what you learned FROM building, not what you planned before
- Use AI to refactor exploratory code into a maintainable, structured codebase

**Key Talking Points:**

- The graduation moment: what are the signals that you've validated enough? Convergent user feedback, repeated patterns across builds, stakeholder alignment, or simply the gut feeling that "this is the one." This judgment call is what separates senior practitioners from beginners. Nobody else teaches this -- it's our biggest differentiation.
- The 73% failure stat: research shows 73% of vibe-coded apps fail to reach production. The #1 reason: teams never deliberately transition from exploration to structure. 8x code duplication, 153% more architectural problems, "comprehension debt" where you build faster than you understand. The graduation moment is the antidote.
- Living specs: the PRD is no longer a document you write *before* building -- it's a document that captures what you've learned *from* building. Show how to extract a PRD from a working prototype: what does this do, who is it for, what assumptions did it validate, what's the recommended architecture going forward? (Directly addresses the old course's "Simplified PRD vs PRO" confusion.)
- Refactoring vibe code: your exploratory prototype works but it's messy. How to use AI tools to restructure it -- component architecture, naming conventions, separation of concerns. The goal: hand someone this codebase and they can understand it in 15 minutes.
- **Graduating within an existing product:** When your prototype proves a feature works within an existing product, the living spec looks different. It's a feature spec that references existing architecture, not a standalone product spec. It needs to answer: what do we add, what do we change, and what existing components does this touch? This is the bridge from "PM prototype" to "engineering ticket."

**Lab: The Graduation Sprint (40 min):**

- **Individual build (25 min):** Each student takes their M3 prototype and "graduates" it:
  1. Write the living PRD -- extract from what they built, not from a template. What does it do? What assumption did it validate? What's the recommended direction?
  2. Refactor the codebase using AI -- restructure for clarity, name components properly, separate concerns. The target: an engineer could read this and understand the intent.
  - Students who built with the existing-product brief in M2 write their spec as a feature addition to the existing product, referencing the constraints they worked within.
- **Group share (15 min):** Small groups (3-4) compare their living PRDs. What did each person prioritize? How does a 0-to-1 spec differ from an existing-product feature spec? Deliverable: a clean, documented prototype with a living PRD.

**"Break It" Exercise (10 min):**
Instructor shows a real example of what happens when you never graduate from vibe coding -- a product that grew organically with no structure until it became unmaintainable. 500+ prompts deep, features bolted on top of features, no one can explain how it works. Lesson: vibe coding is the start, not the end.

---

### Module 5: Real-World Complexity -- Integrations, Edge Cases, and the Engineering Handoff

> **Confidence Line position:** Right side. High confidence, now hardening for reality.

**Timing (2 hrs total):** Teaching (15 min) + Lab (40 min) + Break It (10 min) = **65 min structured, 55 min buffer.** Buffer absorbs: API debugging is unpredictable, chaos round recovery discussion, and engineering handoff conversation often generates rich Q&A from students with real engineering team dynamics to navigate.

**Theme:** Your prototype works in a clean environment. Real products don't. This module bridges the gap between "it works on my screen" and "it works in the real world" -- and teaches you how to hand it off to engineering so it actually gets built.

**Core skill:** Making it real -- connecting to real services, handling what goes wrong, and presenting your work to engineers.

**Learning Objectives:**

- Integrate real APIs and external services into a vibe-coded prototype
- Handle authentication, data persistence, and state management
- Design for edge cases and failure modes, not just the happy path
- Present a prototype to engineering with a living spec that accelerates (not replaces) their work

**Key Talking Points:**

- The integration reality: your prototype works in isolation, but real products talk to APIs, store data, handle authentication, and deal with failures. This is where most vibe-coded prototypes stall out -- not because the idea was wrong, but because the builder never made it real enough to test properly.
- API integration patterns: connecting to real services (Supabase, Firebase, third-party APIs) through AI-assisted development. The key insight: you don't need to understand the API deeply -- you need to describe what you want to the AI tool and verify the result works.
- Edge case thinking: what happens when the API is down? When the user enters garbage? When there are 10,000 items instead of 10? Use AI to generate edge case tests. The discipline: before you call it "done," ask "what breaks this?"
- MCP and agentic integrations (brief demo, not a teaching block): Model Context Protocol is where the industry is heading -- AI tools that connect directly to databases, APIs, and services. Show a 2-minute demo of what this looks like. Students don't need to master this now, but they should know it exists and is coming fast.
- Security checklist (reference, not lecture): a 1-page checklist PMs should run before any prototype touches real data -- rate limiting, input validation, auth token handling, data exposure. Provided as a handout, not taught as a topic.
- **The PM-engineering handoff (moved from M4):** Vibe coding changes this conversation fundamentally. You're no longer handing off a spec document and saying "build this." You're handing off a working reference implementation with a living spec and saying "here's what it does, here's what we validated, here's what needs to be production-grade." Show the three artifacts engineers actually want: (1) the working prototype, (2) the living PRD from M4, (3) a list of "what I hacked vs. what needs to be built properly." This is where the Confidence Line meets reality -- you've moved from ambiguity to confidence, and now you're transferring that confidence to the team that will build it for real.

**Lab: The Integration Sprint (40 min):**

- **Individual build (30 min):** Each student takes their prototype (from any prior module) and adds real-world complexity:
  - Add at least one real API integration (auth, data storage, or external service)
  - Identify and handle at least 2 edge cases (what happens when the API is slow? when input is invalid?)
  - Write a brief "engineering handoff note": what's hacked, what's real, what needs production work
- **Chaos round (10 min):** Instructor triggers a "chaos event" (API goes down, data format changes, unexpected input). Students must handle the failure gracefully in their individual builds. Group discussion: what broke, why, and how would you prioritize fixes?

**"Break It" Exercise (10 min):**
Class exercise: instructor deploys a prototype with zero error handling. Students, as a group, list all the ways a real user could break it. Discussion: how do you prioritize which edge cases to handle when you can't handle them all?

---

### Module 6: Ship It -- Deployment, Scale, and Stakeholder Buy-In

> **Confidence Line position:** Far right. Maximum confidence. Time to ship and present.

**Timing (2 hrs total):** Lab/build day (100 min) + Break It embedded in presentations = **100 min structured, 20 min buffer.** M6 is intentionally tight -- it's a build day, not a teaching day. Buffer absorbs: deployment issues, presentation overruns. **Large cohort contingency:** If cohort > 30 students, limit presentations to 4 groups (selected by peer vote); remaining groups do a gallery walk where classmates visit each deployed URL and leave feedback.

**Theme:** A prototype that lives on your laptop changes nothing. This module takes your work from "demo" to "deployed" and teaches you how to present it for maximum impact.

**Learning Objectives:**

- Deploy a vibe-coded product to a live URL using modern deployment tools
- Understand the basics of scale: what happens when real users hit your product
- Present your product with a compelling narrative that drives stakeholder decisions
- Build a personal vibe coding practice for continued use after the course

**Key Talking Points:**

- Deployment demystified: from localhost to live URL in minutes (Vercel, Netlify, Supabase hosting, etc.)
- Scale awareness: what changes when 1 user becomes 100 becomes 10,000 -- connection pools, rate limits, caching (PM-level awareness, not engineering depth)
- Quality gates before shipping: a PM's checklist for "is this production-worthy?" -- error handling coverage, basic security review, performance under load, accessibility baseline. Use AI to run these checks.
- The stakeholder pitch: framing your vibe-coded product as *evidence for a product decision*, not just a "cool demo." Reforge's insight applies: prototypes are questions, not answers. Your pitch should present the evidence, not just the feature.
- Building your ongoing practice: how to integrate vibe coding into your daily PM workflow -- discovery, stakeholder alignment, eng handoff, user testing
- The future of PM-as-builder: agentic AI is making it possible for PMs to maintain and evolve products post-launch, not just prototype them. Multi-agent workflows, background agents, autonomous testing -- preview of where this capability is heading.
- The full Confidence Line revisited: from ambiguity through validation through production -- students map their own journey

**Lab: Group Build Day (100 min):**

This module follows the Agents course model: after 5 modules of individual practice, students come together in small groups (3-4 max) for a full build session. This is where the group project lives.

- **Group build sprint (50-60 min):** Groups receive a project brief and build from zero together. Each member has their own tool instance and works on a different component/feature simultaneously (unlike the old course where one person built and others watched). They coordinate through a shared living PRD.
- **Deploy (10 min):** Each group deploys their product to a live URL. Real deployment, real URL, shareable with anyone.
- **Present and compete (30 min):** Each group presents with a structured pitch:
  1. The problem (what ambiguity they started with)
  2. The validation journey (what they learned by building)
  3. The product (live demo of the deployed app)
  4. The recommendation (ship it, pivot, or kill it -- and why)
- Class votes on best product AND best pitch (addressing the student feedback requesting a competition element)

**"Break It" Exercise (embedded in presentations):**
During each presentation, the audience tries to break the live deployed product. Teams must respond to failures live -- demonstrating real product resilience.

---

**Group Deliverables (what each team produces):**

1. **Deployed product** -- a live URL, shareable with anyone. This is the artifact, not the outcome.
2. **Validation Evidence Brief** -- a 1-page document that captures: what assumption did we start with, what did we build to test it, what did we learn, and what's our recommendation (ship / pivot / kill) with evidence. This is what a PM would actually hand to their VP on Monday morning. The *real* deliverable.
3. **Living PRD** -- the spec they've been evolving since M4, now attached to the final product. Shows the journey from messy exploration to structured intent.

**Individual Deliverables (what each student takes home):**

1. **Personal project** -- built individually across M1-M5, theirs to keep developing after the course.
2. **Prompt library / living prompt pack** -- built across M3-M5, a reusable toolkit they can apply to any product initiative.
3. **Confidence Line self-assessment** -- where are their real work initiatives on the line? What's stuck in ambiguity that could be moved right with a 2-hour build? What's been "vibed" too long and needs graduation? This is the bridge from course to daily practice.

---

## Appendix A: Feedback-to-Solution Mapping

| Feedback Issue | Solution in New Course |
| --- | --- |
| "Felt like a Lovable tutorial" | Tool-agnostic with Recommended Tool Path; Lovable is default for demos but students choose their tool |
| "Too basic for senior PMs" | No intro AI content; advanced frameworks from minute one; pre-work handles setup |
| "Too much lecture, not enough hands-on" | 80/20 hands-on ratio; labs dominate every module; per-module buffer time ensures labs never get cut |
| "Groups too large / only one person builds" | Individual builds in M1-M5; groups of 3-4 max in M6 |
| "No non-happy-path scenarios" | "Break It" exercise in every module |
| "Want existing product use cases" | Existing product lens threaded through M2 (problem briefs), M3 (context layering), M4 (graduation within existing products), not isolated in one module |
| "PRD confusion" | Living specs concept replaces static PRD; extracted from builds, not written before |
| "Need prototype-to-production path" | M4 (graduation + living spec) -> M5 (integrations + engineering handoff) -> M6 (deployment) |
| "Instructor demo needs to be exciting" | M1 opens with 3-prototype ambiguity demo |
| "Credit/subscription issues" | Lovable Pro partnership for accounts; pre-work verifies setup before Day 1; tool-agnostic removes single-tool dependency |
| "Want competition element" | M6 includes pitch competition with audience voting |
| "Modules felt disconnected" | Confidence Line narrative threads every module together |
| "Content depth was tool-specific" | Depth comes from methodology, not tool mechanics |
| "First hour was all setup" | Pre-work module eliminates setup friction; M1 starts building immediately |
| "Instructor didn't follow the plan" | Instructor Guide with minute-by-minute timing, transition scripts, and "skip if running behind" guidance |
| "No way to measure if changes helped" | Success metrics framework with leading, lagging, and impact indicators |

## Appendix B: Competitive Landscape Detail

**Reforge (AI Prototyping + Build tool)**

- Best insight: "Toy vs Tool" prototype framework (Ravi Mehta). Prototypes should be *decision-making instruments*, not impressive demos.
- Best methodology: 4-step process -- Frame problem with constraints, Generate divergent solutions, Pressure test, Get feedback. The 6-element problem frame (Goal, Problem, Context, Constraints, Success Criteria, Explore) is excellent scaffolding.
- Teaching format: 3-5 hrs/week, mix of self-paced + live case studies with featured guests.
- **Their gap:** Assumes the problem is already validated. Skips the ambiguity stage entirely. Stops at prototype -- no path to production.

**Maven (Multiple competing courses)**

- Vladimir Kalmykov: 20% theory / 80% hands-on with 200+ exercises. 4.8/5 ratings.
- Harold Dijkstra: Vibe Coding Bootcamp ships real products with auth, database, Stripe.
- Prashanth Padmanabhan: "Build your first AI app in 2 hours" -- powerful hook.
- **Their gap:** Either beginner-focused or tool-specific. None teaches the strategic progression from ambiguity to production.

**Harvard Business School (Ideation & Prototyping for Innovation)**

- "Learning by doing" with team projects. AI tools as accelerants for research, not the main event.
- Startups as "experimentation machines" framing. Human-centered design methods embedded.
- **Their gap:** Academic pacing. Not practitioner-focused. Doesn't address tooling, debugging, or PM-eng handoff.

**Coursera/Scrimba (Vibe Coding Essentials)**

- Covers tools broadly: Cursor, GitHub Copilot, Claude Code, MCP.
- **Their gap:** Pure tool training. Zero product development methodology.

**Zero to Mastery (Vibe Coding Bootcamp)**

- "Creative Director" framing -- direct AI tools like a tech lead. 158 lessons, 18 hours.
- **Their gap:** Developer-focused. No validation or product strategy layer.

## Appendix C: Timeline and Working Principles

**Immediate timeline:**

- Wednesday Feb 11: Draft outline shared with Dana
- Friday Feb 13: Validation meeting with Carlos (high-level outline + new vision)
- Following week: Iterate on skeleton, begin detailing Module 1
- Feb 24: Dejan starts teaching AI Agents course (bandwidth decreases)

**Working principles:**

- Reuse over recreate -- preserve the 50-60% of existing content that has good bones
- Validate the skeleton before going into module details
- Repo serves as a living content library Dana can maintain and iterate on going forward
- Content must be instructor-proof -- labs are self-guiding with clear step-by-step flows and timing

**Source documents:**

- Insights/AI Prototyping V3 Refinement.txt -- Multi-cohort feedback synthesis and module-by-module diagnosis
- Insights/2025-02-09 Dana-Dejan Kickoff Meeting.md -- Internal alignment on vision, lab structure, and working approach

## Appendix D: Success Metrics -- Measurement Details

Full success metrics are defined in the main body (see **"How We Know It's Working"** section above). The Instructor Guide contains the detailed rubrics for evaluating Validation Evidence Briefs, prompt libraries, and living PRDs, as well as the per-module timing log template for tracking hands-on time ratios.

## Appendix E: Content Reuse Map

This maps the old AI Prototyping course modules to the new Vibe Coding course, identifying what content can be reused, what should be discarded, and what needs to evolve. Estimated overall reuse: **~55-60%** of existing slide/lab content can be adapted.

### Old M1: Accelerate the Idea-to-Validation Cycle

| Content | Action | Maps To |
| --- | --- | --- |
| Validation cycle concept | **Reuse** | New M1 (opening demo concept) + New M2 (validation framing) |
| "Aha moment" live demo concept | **Reuse** | New M1 opening demo (but make it 3 prototypes, not 1) |
| Introductory AI slides ("why AI is useful") | **Discard** | Senior PMs don't need this |
| "Why prototyping matters" content | **Discard** | Replaced by immediate action in M1 |
| Lab: first prototype build | **Evolve** | New M1 lab (individual, not group; ambiguous brief, not structured) |

### Old M2: Write Advanced Prompts for High-Fidelity Prototypes

| Content | Action | Maps To |
| --- | --- | --- |
| FUDI framework for prompting | **Reuse** | New M3 (Precision Prompting) -- reframe as one of several techniques |
| Chain-of-Thought, Self-Consistency techniques | **Reuse** | New M3 advanced techniques section |
| Basic prompting intro (System vs User prompts) | **Discard** | Too basic for this audience |
| Token window explanations | **Discard** | Tool-specific, outdated quickly |
| "Guessing exercise" | **Discard** | Low-value per student feedback |
| Lab: prompt modification exercise | **Evolve** | New M3 lab (document prompt chain, peer review for portability) |

### Old M3: Define the Modern Specs for Prototype Development

| Content | Action | Maps To |
| --- | --- | --- |
| PRD templates and spec structure | **Reuse** | New M4 (Living specs) -- reframe as extracted from builds, not written before |
| "Simplified PRD vs PRO" distinction | **Discard** | Caused confusion; replaced by single "living PRD" concept |
| Static PRD approach | **Discard** | Replaced by living document concept |
| Field examples of PM team organization | **Reuse** | New M4 talking points + New M5 engineering handoff |
| Lab: PRD creation | **Evolve** | New M4 lab (extract PRD from working prototype, not write from scratch) |

### Old M4: Build, Iterate, and Debug End-to-End

| Content | Action | Maps To |
| --- | --- | --- |
| Debugging techniques | **Evolve** | Lighter version folded into New M3 (prompt debugging) |
| Iterative build patterns | **Reuse** | New M3 + M4 (iterative refinement) |
| Chat Mode vs Agent Mode distinction | **Evolve** | Brief mention in Pre-Work (mode overview) + M3 (agentic prompting); no tool-specific UI walkthrough |
| FUDI vs Prompt Pack confusion | **Discard** | Replaced by unified "living prompt pack" concept in M3 |
| Lab: instructor-led build (students watch) | **Discard** | Replaced by individual builds throughout |

### Old M5: Power Advanced Prototypes with API Integrations

| Content | Action | Maps To |
| --- | --- | --- |
| API integration patterns (Supabase, Firebase) | **Reuse** (~70% directly) | New M5 (Real-World Complexity) |
| Auth setup walkthrough | **Reuse** | New M5 integration lab |
| Step-by-step Lovable-specific API instructions | **Discard** | Tool-specific; will break when UI changes |
| Lab: API integration exercise | **Evolve** | New M5 lab (add complexity to YOUR prototype, not a canned exercise) |

### Old M6: Deploy and Scale Prototypes for Production

| Content | Action | Maps To |
| --- | --- | --- |
| Deployment flow (Vercel/Netlify) | **Reuse** | New M6 (Ship It) |
| Basic scale concepts | **Reuse** | New M6 talking points |
| Group project structure | **Evolve** | New M6 (group build day with parallel individual builds, not one-person-builds) |
| Final presentation format | **Evolve** | New M6 (add competition element, structured pitch format, Validation Evidence Brief) |
